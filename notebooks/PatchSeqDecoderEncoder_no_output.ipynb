{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import copy as copy\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.configs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partitions(celltype, n_partitions, seed=0):\n",
    "    \"\"\"\n",
    "    adapted from https://github.com/AllenInstitute/coupledAE-patchseq\n",
    "    \"\"\"\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "    # Safe to ignore warning - there are celltypes with a low sample number that are not crucial for the analysis.\n",
    "    with warnings.catch_warnings():\n",
    "        skf = StratifiedKFold(n_splits=n_partitions, random_state=seed, shuffle=True)\n",
    "\n",
    "    # Get all partition indices from the sklearn generator:\n",
    "    ind_dict = [{'train': train_ind, 'val': val_ind} for train_ind, val_ind in\n",
    "                skf.split(X=np.zeros(shape=celltype.shape), y=celltype)]\n",
    "    return ind_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_ps(adata_rna_raw,adata_ephys_raw,adata_morph_raw,cv,split=False):\n",
    "  adata_rna,adata_ephys,adata_morph = adata_rna_raw.copy(),adata_ephys_raw.copy(),adata_morph_raw.copy()\n",
    "  adatas_train,adatas_test = [],[]\n",
    "  assert (adata_rna.X>=0).all(), \"poluted input\"\n",
    "  for mod in [adata_rna,adata_ephys,adata_morph]:\n",
    "    mod.obs['label'] = mod.obs['cell_type_TEM']\n",
    "    if split:\n",
    "      m_train = mod[ind_dict[cv]['train']]\n",
    "      scaler = preprocessing.StandardScaler().fit(m_train.X)\n",
    "      m_train.X = scaler.transform(m_train.X)\n",
    "\n",
    "      m_test = mod[ind_dict[cv]['val']]\n",
    "      scaler = preprocessing.StandardScaler().fit(m_test.X)\n",
    "      m_test.X = scaler.transform(m_test.X)\n",
    "    else:\n",
    "      scaler = preprocessing.StandardScaler().fit(mod.X)\n",
    "      mod.X = scaler.transform(mod.X)\n",
    "      m_train = mod[ind_dict[cv]['train']]\n",
    "      m_test = mod[ind_dict[cv]['val']]\n",
    "\n",
    "    adatas_train.append(m_train)\n",
    "    adatas_test.append(m_test)\n",
    "  adatas_all = [ad.concat([m_train,m_test]) for m_train,m_test in zip(adatas_train,adatas_test)]\n",
    "  return adatas_train,adatas_test,adatas_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.encoder_decoder_only.encoder_decoder_only_model import EncoderDecoderOnlyUnitedNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data set ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to split training : testing data size\n",
    "k_folds=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "technique = 'patchseq'\n",
    "data_path = f\"../data/{technique}\"\n",
    "device = \"mps\"\n",
    "#load data\n",
    "adata_rna_raw = sc.read_h5ad(f'{data_path}/adata_RNA_TEM.h5ad')\n",
    "adata_ephys_raw = sc.read_h5ad(f'{data_path}/adata_Ephys_TEM.h5ad')\n",
    "adata_morph_raw = sc.read_h5ad(f'{data_path}/adata_Morph_TEM.h5ad')\n",
    "ind_dict = partitions(adata_rna_raw.obs['cell_type_TEM'], n_partitions=k_folds, seed=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_save_path = f\"../saved_results/encoder_decoder/{technique}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cv in range(k_folds):\n",
    "  adatas_train,adatas_test,_ = pre_ps(adata_rna_raw,adata_ephys_raw,adata_morph_raw,cv,split=True)\n",
    "  root_save_path = f\"./saved_results/encoder_decoder/{technique}_{cv}\"\n",
    "  model = EncoderDecoderOnlyUnitedNet(root_save_path, device=device, technique=encoder_decoder_only_patchseq_config)\n",
    "  print(model.model.config[str_train_task])\n",
    "  model.train(adatas_train,adatas_val = adatas_test, verbose=True)\n",
    "\n",
    "  print(model.evaluate(adatas_test))\n",
    "  adata_last_fold = adatas_test\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"TRAINING FOR str_supervised_group_identigy_only\")\n",
    "# for cv in range(k_folds):\n",
    "#     _,_,adatas_all = pre_ps(adata_rna_raw,adata_ephys_raw,adata_morph_raw,cv,split=False)\n",
    "#     model.load_model(f\"{root_save_path}/train_best.pt\",device=torch.device(device))\n",
    "#     model.model.device_in_use = device\n",
    "#     model.train(adatas_all,verbose=True,init_classify=True)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "adatas_all = adata_last_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('==============best finetune================')\n",
    "model.load_model(f\"{root_save_path}/train_best.pt\",device=torch.device(device))\n",
    "# model.model.device_in_use = device\n",
    "model.evaluate(adatas_all,give_losses=True,stage=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses = model.evaluate(adatas_all,give_losses=True,stage='train')\n",
    "predict_label = model.predict_label(adatas_all)\n",
    "adata_fused = model.infer(adatas_all)\n",
    "adata_fused.obs['label'] = list(adatas_all[0].obs['label'])\n",
    "adata_fused.obs['label_less'] = [ct.split('-')[0] for ct in adata_fused.obs['label'].values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "\n",
    "pseudo_label = np.array(adata_fused.obs['predicted_label'].values)\n",
    "cmat = confusion_matrix(adata_fused.obs['label'], pseudo_label)\n",
    "ri, ci = linear_sum_assignment(-cmat)\n",
    "ordered_all = cmat[np.ix_(ri, ci)]\n",
    "major_sub_names = {}\n",
    "pred_labels_re_order = copy.deepcopy(pseudo_label)\n",
    "for re_oder,(lb_correct,lb) in enumerate(zip(unique_labels(adata_fused.obs['label'], pseudo_label)[ri],\n",
    "                                unique_labels(adata_fused.obs['label'], pseudo_label)[ci])):\n",
    "  idx = pseudo_label==lb\n",
    "  if any(idx):\n",
    "    nm = '-'.join(lb_correct.split('-')[:-1])\n",
    "    if nm in major_sub_names.keys():\n",
    "      major_sub_names[nm]+=1\n",
    "    else:\n",
    "      major_sub_names[nm]=1\n",
    "    \n",
    "    pred_labels_re_order[idx] = f'{nm}-{major_sub_names[nm]}-Uni'\n",
    "\n",
    "adata_fused.obs['predicted_label'] = pred_labels_re_order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "def ordered_cmat(labels, pred):\n",
    "    \"\"\"\n",
    "    Compute the confusion matrix and accuracy corresponding to the best cluster-to-class assignment.\n",
    "\n",
    "    :param labels: Label array\n",
    "    :type labels: np.array\n",
    "    :param pred: Predictions array\n",
    "    :type pred: np.array\n",
    "    :return: Accuracy and confusion matrix\n",
    "    :rtype: Tuple[float, np.array]\n",
    "    \"\"\"\n",
    "    cmat = confusion_matrix(labels, pred)\n",
    "    ri, ci = linear_sum_assignment(-cmat)\n",
    "    ordered = cmat[np.ix_(ri, ci)]\n",
    "    acc = np.sum(np.diag(ordered))/np.sum(ordered)\n",
    "    return acc, ordered\n",
    "\n",
    "labels = pseudo_label\n",
    "predictions = adata_fused.obs['label']\n",
    "acc, ordered = ordered_cmat(labels, predictions)\n",
    "metrics = {\n",
    "    \"confusion\": ordered,\n",
    "    \"acc\": acc,\n",
    "    \"ari\": adjusted_rand_score(labels, predictions),\n",
    "    \"nmi\": normalized_mutual_info_score(\n",
    "        labels, predictions, average_method=\"geometric\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_labels(pseudo_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pseudo_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_labels(adata_fused.obs['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('ticks')\n",
    "adata_fused.obs['predicted_label_less'] = [ct.split('-')[0] for ct in adata_fused.obs['predicted_label'].values]\n",
    "cmat = confusion_matrix(adata_fused.obs['predicted_label'], adata_fused.obs['label_less'])\n",
    "cmat = cmat[:,cmat.sum(axis=0)!=0]\n",
    "cmat = cmat[cmat.sum(axis=1)!=0,:]\n",
    "cmat = (cmat.T / cmat.sum(axis=1)).T\n",
    "\n",
    "fig,ax = plt.subplots(figsize=[1.2,5])\n",
    "sns.heatmap(cmat,ax=ax,yticklabels=unique_labels(adata_fused.obs['predicted_label']),xticklabels=unique_labels(adata_fused.obs['label_less']),vmin=0, vmax=1)\n",
    "plt.xlabel('TEM joint label')\n",
    "plt.savefig('./figures/major_matching_heatmap.pdf')\n",
    "\n",
    "fig,ax = plt.subplots(figsize=[6,5])\n",
    "ordered = ordered_all[:,ordered_all.sum(axis=0)!=0]\n",
    "ordered = ordered[ordered.sum(axis=1)!=0,:]\n",
    "ordered_re = ordered.T\n",
    "ordered_norm = (ordered_re.T / ordered_re.sum(axis=1)).T\n",
    "\n",
    "sns.heatmap(ordered_norm,ax=ax,xticklabels=unique_labels(adata_fused.obs['label']),yticklabels=unique_labels(adata_fused.obs['predicted_label']),vmin=0, vmax=1)\n",
    "plt.xlabel('TEM joint label')\n",
    "os.makedirs('./figures/', exist_ok=True)\n",
    "plt.savefig('./figures/sub_matching_heatmap.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sc.pl.umap(adata_fused, \n",
    "           color=['label'], \n",
    "           palette='rainbow', \n",
    "           show=True, \n",
    "           edges=True, \n",
    "           edges_width=0.2, \n",
    "           edgecolor='k', \n",
    "           title='', \n",
    "           save='patch_seq_2D_orig_MET.pdf')\n",
    "sc.pl.umap(adata_fused,\n",
    "           color=['predicted_label'],\n",
    "           palette='rainbow',\n",
    "           show=True,\n",
    "           edges=True,\n",
    "           edges_width = 0.2,\n",
    "           edgecolor='k',\n",
    "           title='',\n",
    "           save='patch_seq_2D_Uni_MET.pdf')\n",
    "\n",
    "\n",
    "sc.pl.umap(adata_fused,color=['label_less'],palette='rainbow',show=True,edges=True,edges_width = 0.2,edgecolor='k',title='',save='patch_seq_2D_MET_comparison.pdf')\n",
    "sc.pl.umap(adata_fused,color=['predicted_label_less'],palette='rainbow',show=True,edges=True,edges_width = 0.2,edgecolor='k',title='',save='patch_seq_2D_MET_comparison_no_legend.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP IQ\n",
    "\n",
    "First I will try to recover the current SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapiq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adatas_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fused_latent_codes(adatas):\n",
    "    # Get fused latent codes from the model\n",
    "    from src.data import create_dataloader\n",
    "    dataloader = create_dataloader(model.model, adatas, shuffle=False, batch_size=len(adatas[0]))\n",
    "    \n",
    "    model.model.eval()\n",
    "    with torch.no_grad():\n",
    "        for modalities, labels in dataloader:\n",
    "            outputs = model.model(modalities, labels)\n",
    "            fused_latents = model.model.fused_latents[model.model.best_head]\n",
    "            break\n",
    "    \n",
    "    # Convert to numpy\n",
    "    X = fused_latents.detach().cpu().numpy()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_latent_codes = get_fused_latent_codes(adatas_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_latent_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_fused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_id = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted label\n",
    "adata_fused.obs['predicted_label'][instance_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_fused.obs['label'][instance_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simple prediction function (returns class probabilities)\n",
    "def predict_func(latent_codes):\n",
    "    if latent_codes.ndim == 1:\n",
    "        latent_codes = latent_codes.reshape(1, -1)\n",
    "    \n",
    "    latent_tensor = torch.tensor(latent_codes, dtype=torch.float32).to(model.model.device_in_use)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden = model.model.projectors[model.model.best_head](latent_tensor)\n",
    "        class_outputs = model.model.clusters[model.model.best_head](hidden)\n",
    "        class_probs = model.model.prob_layer(class_outputs)\n",
    "    \n",
    "    return class_probs.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shapiq.TabularExplainer(\n",
    "        model=predict_func,\n",
    "        data=fused_latent_codes,\n",
    "        index=\"SV\",\n",
    "        max_order=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv = explainer.explain(x=fused_latent_codes[instance_id], budget=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index = max(ind_dict[cv]['train'])\n",
    "max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modules import submodel_clus\n",
    "sub = submodel_clus(model.model).to(model.device)\n",
    "# Simple and effective background data\n",
    "unique_labels = np.unique(pseudo_label)\n",
    "cluster_prototype_features = []\n",
    "for ad_x in adatas_all:\n",
    "    type_means = []\n",
    "    for label in unique_labels:\n",
    "        mask = pseudo_label == label\n",
    "        if np.any(mask):\n",
    "            type_mean = np.mean(ad_x.X[mask], axis=0)\n",
    "            type_means.append(type_mean)\n",
    "    cluster_prototype_features.append(torch.tensor(np.array(type_means), device=model.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = shap.DeepExplainer(sub, cluster_prototype_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 3: Following the exact PatchSeq pattern\n",
    "modality_to_explain = 0  # Start with first modality (e.g., RNA)\n",
    "test_type = torch.tensor(adatas_all[modality_to_explain].X, device=model.device)\n",
    "shap_values = e.shap_values([test_type], check_additivity=True)\n",
    "\n",
    "print(f\"SHAP values shape: {shap_values.shape}\")\n",
    "print(f\"Explained modality {modality_to_explain}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = [torch.tensor(ad_x.X, device=model.device) for ad_x in adatas_all]\n",
    "shap_values = e.shap_values(test_sample, check_additivity=False)\n",
    "print(f\"Done! Shape: {len(shap_values)} modalities\")\n",
    "for i, sv in enumerate(shap_values):\n",
    "    print(f\"Modality {i}: {sv.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_shap_to_features_fixed(shap_values, adatas, sample_idx=0, class_idx=None):\n",
    "    \"\"\"\n",
    "    Map SHAP values back to original feature names\n",
    "    \n",
    "    Args:\n",
    "        shap_values: SHAP values - list of arrays [modality] with shape (samples, features, classes)\n",
    "        adatas: List of AnnData objects for each modality\n",
    "        sample_idx: Which sample to analyze\n",
    "        class_idx: Which class to analyze (if None, uses mean across all classes)\n",
    "    \"\"\"\n",
    "    modality_names = ['RNA', 'Ephys', 'Morph']\n",
    "    results = {}\n",
    "\n",
    "    for mod_idx, (shap_mod, adata, mod_name) in enumerate(zip(shap_values, adatas, modality_names)):\n",
    "        \n",
    "        # shap_mod has shape (n_samples, n_features, n_classes)\n",
    "        if class_idx is not None:\n",
    "            # Use specific class\n",
    "            sample_shap = shap_mod[sample_idx, :, class_idx]  # Shape: (n_features,)\n",
    "        else:\n",
    "            # Average across all classes (mean absolute SHAP values)\n",
    "            sample_shap = np.mean(np.abs(shap_mod[sample_idx, :, :]), axis=1)  # Shape: (n_features,)\n",
    "\n",
    "        # Get feature names\n",
    "        if hasattr(adata, 'var_names'):\n",
    "            feature_names = list(adata.var_names)\n",
    "        elif hasattr(adata, 'var') and 'gene_symbols' in adata.var.columns:\n",
    "            feature_names = list(adata.var['gene_symbols'])\n",
    "        else:\n",
    "            feature_names = [f\"{mod_name}_Feature_{i}\" for i in range(len(sample_shap))]\n",
    "\n",
    "        # Create feature importance list\n",
    "        feature_importance = []\n",
    "        for i, (shap_val, feature_name) in enumerate(zip(sample_shap, feature_names)):\n",
    "            if hasattr(shap_val, 'item'):\n",
    "                shap_scalar = shap_val.item()\n",
    "            else:\n",
    "                shap_scalar = float(shap_val)\n",
    "\n",
    "            feature_importance.append({\n",
    "                'feature_name': feature_name,\n",
    "                'shap_value': shap_scalar,\n",
    "                'abs_shap_value': abs(shap_scalar),\n",
    "                'feature_index': i\n",
    "            })\n",
    "\n",
    "        # Sort by absolute SHAP value\n",
    "        feature_importance.sort(key=lambda x: x['abs_shap_value'], reverse=True)\n",
    "        results[mod_name] = feature_importance\n",
    "\n",
    "    return results\n",
    "\n",
    "# Now you can use the original analyze_pvalb_features function:\n",
    "def analyze_pvalb_features(shap_values, adatas_all, pseudo_label, cell_type_prefix='Pvalb', top_n=10):\n",
    "    \"\"\"\n",
    "    Analyze which features are most important for a specific cell type\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get indices of neurons with the specified prefix\n",
    "    print(len(pseudo_label))\n",
    "    cell_type_mask = np.array([label.startswith(cell_type_prefix) for label in pseudo_label])\n",
    "    cell_type_indices = np.where(cell_type_mask)[0]\n",
    "    \n",
    "    if len(cell_type_indices) == 0:\n",
    "        print(f\"No {cell_type_prefix} neurons found!\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Found {len(cell_type_indices)} {cell_type_prefix} neurons\")\n",
    "    \n",
    "    # Show the subtypes found\n",
    "    subtypes = set([label for label in pseudo_label if label.startswith(cell_type_prefix)])\n",
    "    print(f\"Subtypes: {subtypes}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Analyze each neuron and aggregate results\n",
    "    all_features = {mod: [] for mod in ['RNA', 'Ephys', 'Morph']}\n",
    "    \n",
    "    for neuron_idx in cell_type_indices:\n",
    "        # Get SHAP mapping for this neuron using the fixed function\n",
    "        mapped_shap = map_shap_to_features_fixed(shap_values, adatas_all, \n",
    "                                               sample_idx=neuron_idx, class_idx=None)\n",
    "        \n",
    "        # Collect features from each modality\n",
    "        for modality in mapped_shap:\n",
    "            all_features[modality].extend(mapped_shap[modality])\n",
    "    \n",
    "    # Rest remains the same...\n",
    "    aggregated_features = {}\n",
    "    \n",
    "    for modality in all_features:\n",
    "        if not all_features[modality]:\n",
    "            continue\n",
    "            \n",
    "        feature_groups = {}\n",
    "        for feature_data in all_features[modality]:\n",
    "            feature_name = feature_data['feature_name']\n",
    "            if feature_name not in feature_groups:\n",
    "                feature_groups[feature_name] = []\n",
    "            feature_groups[feature_name].append(feature_data['abs_shap_value'])\n",
    "        \n",
    "        aggregated = []\n",
    "        for feature_name, shap_values_list in feature_groups.items():\n",
    "            mean_shap = np.mean(shap_values_list)\n",
    "            aggregated.append({\n",
    "                'feature_name': feature_name,\n",
    "                'mean_abs_shap': mean_shap,\n",
    "                'n_neurons': len(shap_values_list)\n",
    "            })\n",
    "        \n",
    "        aggregated.sort(key=lambda x: x['mean_abs_shap'], reverse=True)\n",
    "        aggregated_features[modality] = aggregated\n",
    "    \n",
    "    # Print results\n",
    "    for modality in ['RNA', 'Ephys', 'Morph']:\n",
    "        if modality in aggregated_features:\n",
    "            print(f\"\\nTop {top_n} features for {modality} ({cell_type_prefix}-specific):\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            for i, feature in enumerate(aggregated_features[modality][:top_n]):\n",
    "                print(f\"{i+1:2d}. {feature['feature_name']:25s} | \"\n",
    "                      f\"Mean |SHAP|: {feature['mean_abs_shap']:8.4f}\")\n",
    "    \n",
    "    return aggregated_features\n",
    "\n",
    "\n",
    "pvalb_features = analyze_pvalb_features(shap_values, adatas_all, pseudo_label, \n",
    "                                       cell_type_prefix='Pvalb', top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the device mismatch issue\n",
    "def fix_model_device(model, target_device='mps'):\n",
    "    \"\"\"\n",
    "    Ensure all model components are on the same device\n",
    "    \"\"\"\n",
    "    print(f\"Moving model to {target_device}...\")\n",
    "    \n",
    "    # Move the main model\n",
    "    model.model = model.model.to(target_device)\n",
    "    \n",
    "    # Explicitly move all subcomponents\n",
    "    model.model.encoders = model.model.encoders.to(target_device)\n",
    "    model.model.decoders = model.model.decoders.to(target_device)\n",
    "    model.model.fusers = model.model.fusers.to(target_device)\n",
    "    model.model.latent_projector = model.model.latent_projector.to(target_device)\n",
    "    model.model.projectors = model.model.projectors.to(target_device)\n",
    "    model.model.clusters = model.model.clusters.to(target_device)\n",
    "    \n",
    "    # Update device_in_use\n",
    "    model.model.device_in_use = target_device\n",
    "    \n",
    "    # Verify all parameters are on the correct device\n",
    "    for name, param in model.model.named_parameters():\n",
    "        if param.device.type != target_device:\n",
    "            print(f\"Warning: {name} is on {param.device}, moving to {target_device}\")\n",
    "            param.data = param.data.to(target_device)\n",
    "    \n",
    "    print(\"Model device fix complete!\")\n",
    "    return model\n",
    "\n",
    "# Apply the fix\n",
    "model = fix_model_device(model, target_device='mps')\n",
    "\n",
    "# Now try inference again\n",
    "all_adata_fused = model.infer(_)\n",
    "all_adata_fused.obs['label'] = list(_[0].obs['label'])\n",
    "all_adata_fused.obs['label_less'] = [ct.split('-')[0] for ct in all_adata_fused.obs['label'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_adata_fused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "all_pseudo_label = np.array(all_adata_fused.obs['predicted_label'].values)\n",
    "cmat = confusion_matrix(all_adata_fused.obs['label'], all_pseudo_label)\n",
    "ri, ci = linear_sum_assignment(-cmat)\n",
    "ordered_all = cmat[np.ix_(ri, ci)]\n",
    "major_sub_names = {}\n",
    "pred_labels_re_order = copy.deepcopy(all_pseudo_label)\n",
    "for re_oder,(lb_correct,lb) in enumerate(zip(unique_labels(all_adata_fused.obs['label'], all_pseudo_label)[ri],\n",
    "                               unique_labels(all_adata_fused.obs['label'], all_pseudo_label)[ci])):\n",
    " idx = all_pseudo_label==lb\n",
    " if any(idx):\n",
    "   nm = '-'.join(lb_correct.split('-')[:-1])\n",
    "   if nm in major_sub_names.keys():\n",
    "     major_sub_names[nm]+=1\n",
    "   else:\n",
    "     major_sub_names[nm]=1\n",
    "   \n",
    "   pred_labels_re_order[idx] = f'{nm}-{major_sub_names[nm]}-Uni'\n",
    "\n",
    "all_adata_fused.obs['predicted_label'] = pred_labels_re_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = [torch.tensor(ad_x.X, device=model.device) for ad_x in _]\n",
    "shap_values = e.shap_values(test_sample, check_additivity=False)\n",
    "print(f\"Done! Shape: {len(shap_values)} modalities\")\n",
    "for i, sv in enumerate(shap_values):\n",
    "    print(f\"Modality {i}: {sv.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all cells\n",
    "analyze_pvalb_features(shap_values, _, all_pseudo_label, \n",
    "                                       cell_type_prefix='Pvalb', top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapiq\n",
    "import torch\n",
    "import numpy as np\n",
    "from src.modules import submodel_clus\n",
    "\n",
    "# 1. Extract the clustering submodel\n",
    "clustering_submodel = submodel_clus(model.model).to(model.device)\n",
    "\n",
    "# 2. Create a wrapper for the submodel\n",
    "def submodel_wrapper(X):\n",
    "    \"\"\"\n",
    "    Wrapper to make submodel_clus compatible with SHAPIQ\n",
    "    X: numpy array of shape (n_samples, total_features)\n",
    "    Returns: numpy array of predictions\n",
    "    \"\"\"\n",
    "    batch_size = X.shape[0]\n",
    "    \n",
    "    # Split X back into modalities based on your original dimensions\n",
    "    # You'll need to know the feature dimensions for each modality\n",
    "    modality_dims = [1252, 68, 514]  # Replace with your actual dimensions from config\n",
    "    \n",
    "    modalities = []\n",
    "    start_idx = 0\n",
    "    for dim in modality_dims:\n",
    "        end_idx = start_idx + dim\n",
    "        modality_data = X[:, start_idx:end_idx]\n",
    "        modalities.append(torch.tensor(modality_data, dtype=torch.float32, device=model.device))\n",
    "        start_idx = end_idx\n",
    "    \n",
    "    # Run through the clustering submodel\n",
    "    clustering_submodel.eval()\n",
    "    with torch.no_grad():\n",
    "        # submodel_clus.forward() expects individual modality tensors\n",
    "        predictions = clustering_submodel(*modalities)\n",
    "        \n",
    "        # Convert to numpy\n",
    "        if hasattr(predictions, 'cpu'):\n",
    "            predictions = predictions.cpu().numpy()\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# 3. Prepare your data (same as before)\n",
    "def prepare_data_for_shapiq(adatas):\n",
    "    \"\"\"Convert AnnData objects to flattened numpy array\"\"\"\n",
    "    all_features = []\n",
    "    for adata in adatas:\n",
    "        if hasattr(adata.X, 'toarray'):\n",
    "            features = adata.X.toarray()\n",
    "        else:\n",
    "            features = adata.X\n",
    "        all_features.append(features)\n",
    "    \n",
    "    X = np.concatenate(all_features, axis=1)\n",
    "    return X\n",
    "\n",
    "# # 4. Set up the explainer with the submodel\n",
    "# X = prepare_data_for_shapiq(_)\n",
    "\n",
    "# explainer = shapiq.TabularExplainer(\n",
    "#     model=submodel_wrapper,\n",
    "#     data=X,\n",
    "#     index=\"SV\",  # or \"k-SII\"\n",
    "#     max_order=1\n",
    "# )\n",
    "\n",
    "# # 5. Compute explanations\n",
    "# explanations = explainer.explain(X[0], budget=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapiq\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 1. Create a simple wrapper for just ephys data\n",
    "class EphysSubmodelWrapper:\n",
    "    def __init__(self, full_model, device):\n",
    "        # Extract just the ephys encoder and relevant parts\n",
    "        self.device = device\n",
    "        self.encoder = full_model.encoders[1]  # Ephys is index 1\n",
    "        self.fuser = full_model.fusers[full_model.best_head]  # Use best head\n",
    "        self.projector = full_model.projectors[full_model.best_head]\n",
    "        self.cluster = full_model.clusters[full_model.best_head]\n",
    "        self.prob_layer = full_model.prob_layer\n",
    "        \n",
    "    def __call__(self, X):\n",
    "        \"\"\"\n",
    "        X: numpy array of shape (n_samples, n_ephys_features)\n",
    "        Returns: numpy array of predictions\n",
    "        \"\"\"\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Encode ephys data\n",
    "            latent = self.encoder(X)\n",
    "            \n",
    "            # For single modality, we need to simulate the fusing step\n",
    "            # The fuser expects multiple modalities, so we'll just use the ephys latent\n",
    "            # or bypass fusing for single modality\n",
    "            \n",
    "            # Project to clustering space\n",
    "            hidden = self.projector(latent)\n",
    "            \n",
    "            # Get cluster outputs\n",
    "            cluster_output = self.cluster(hidden)\n",
    "            \n",
    "            # Convert to probabilities\n",
    "            probs = self.prob_layer(cluster_output)\n",
    "            \n",
    "            # Return class predictions or probabilities\n",
    "            predictions = torch.argmax(probs, dim=1)  # Class predictions\n",
    "            \n",
    "            return predictions.cpu().numpy()\n",
    "\n",
    "# 2. Prepare just the ephys data\n",
    "def prepare_ephys_data(adatas):\n",
    "    \"\"\"Extract just the ephys data (modality index 1)\"\"\"\n",
    "    ephys_adata = adatas[1]  # Ephys is typically index 1\n",
    "    \n",
    "    if hasattr(ephys_adata.X, 'toarray'):\n",
    "        X = ephys_adata.X.toarray()\n",
    "    else:\n",
    "        X = ephys_adata.X\n",
    "    \n",
    "    return X\n",
    "\n",
    "# 3. Set up the explainer\n",
    "# Prepare your ephys data\n",
    "X_ephys = prepare_ephys_data(_)\n",
    "\n",
    "# Create the wrapper\n",
    "ephys_wrapper = EphysSubmodelWrapper(model.model, model.device)\n",
    "\n",
    "# Create explainer\n",
    "explainer = shapiq.TabularExplainer(\n",
    "    model=ephys_wrapper,\n",
    "    data=X_ephys,\n",
    "    index=\"SV\",  # Start with Shapley Interaction Index\n",
    "    max_order=1   # Start with pairwise interactions\n",
    ")\n",
    "\n",
    "# # 4. Compute explanations for a subset\n",
    "# print(f\"Ephys data shape: {X_ephys.shape}\")\n",
    "# explanations = explainer.explain(X_ephys[0], budget=256)  # Explain first 10 samples\n",
    "\n",
    "# explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type_mask = np.array([label.startswith('Pvalb') for label in all_pseudo_label])\n",
    "cell_type_indices = np.where(cell_type_mask)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = explainer.explain(X_ephys[0], budget=256)  # Explain first 10 samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation.plot_force(feature_names=adata_ephys_raw.var_names, abbreviate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations = []\n",
    "explainer = shapiq.TabularExplainer(\n",
    "    model=ephys_wrapper,\n",
    "    data=X_ephys,\n",
    "    index=\"SV\",  # Start with Shapley Interaction Index\n",
    "    max_order=1   # Start with pairwise interactions\n",
    ")\n",
    "for instance_id in cell_type_indices:\n",
    "    x_explain = X_ephys[instance_id]\n",
    "    si = explainer.explain(x=x_explain, budget=256)\n",
    "    explanations.append(si)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_explanations(explainer: shapiq.Explainer, indices, X, budget=256):\n",
    "    explanations = []\n",
    "    for instance_id in indices:\n",
    "        x_explain = X[instance_id]\n",
    "        si = explainer.explain(x=x_explain, budget=budget)\n",
    "        explanations.append(si)\n",
    "    return explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalb_explanations = get_explanations(explainer, cell_type_indices, X_ephys, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiq.plot.bar_plot(pvalb_explanations, feature_names=adata_ephys_raw.var_names, show=True, abbreviate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiq.plot.bar_plot(explanations, feature_names=adata_ephys_raw.var_names, show=True, abbreviate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ephys.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to reproduce with KernalSHAP estimation ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapiq.approximator import KernelSHAP\n",
    "approximator = KernelSHAP(n=68) # 68 Ephys features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernal_shap_explainer = shapiq.TabularExplainer(\n",
    "    model=ephys_wrapper,\n",
    "    data=X_ephys,\n",
    "    approximator=approximator,\n",
    "    index=\"SV\",\n",
    "    max_order=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalb__kernal_shap_explanations = get_explanations(kernal_shap_explainer, cell_type_indices, X_ephys, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiq.plot.bar_plot(pvalb__kernal_shap_explanations, feature_names=adata_ephys_raw.var_names, show=True, abbreviate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapiq.approximator import SHAPIQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "approximator = SHAPIQ(n=68, max_order=2, index='k-SII') # 68 Ephys features\n",
    "kernal_shap_explainer_2_order = shapiq.TabularExplainer(\n",
    "    model=ephys_wrapper,\n",
    "    data=X_ephys,\n",
    "    approximator=approximator,\n",
    "    index=\"k-SII\",\n",
    "    max_order=2,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With approximator, budget = 500, took 1 minute 19 seconds\n",
    "# With no approximator, same budget, took 1 minute 24 seconds, the number of coalitions calculated is a bit smaller, 6.12 vs 6.34. Results are the same\n",
    "si = kernal_shap_explainer_2_order.explain(x=X_ephys[5], budget=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = all_adata_fused.obs['label'].values\n",
    "print(f\"Cell type is {all_pseudo_label[5]} for predicted label\")\n",
    "print(f\"Cell type is {true_labels[5]} for true label\")\n",
    "shapiq.plot.bar_plot([si], feature_names=adata_ephys_raw.var_names, show=True, abbreviate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# si.plot_network(feature_names=adata_ephys_raw.var_names, n_interactions=10)\n",
    "shapiq.plot.si_graph_plot(si, draw_threshold=3.15, show=True, plot_explanation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP IQ With Approximator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from shapiq.approximator import KernelSHAPIQ\n",
    "shapiq_approximator_2_order = KernelSHAPIQ(n=n, max_order=2, index=\"k-SII\")\n",
    "\n",
    "explainer = shapiq.TabularExplainer(\n",
    "    model=submodel_wrapper,\n",
    "    data=X,\n",
    "    approximator=shapiq_approximator_2_order,\n",
    "    index=\"k-SII\",  # or \"k-SII\"\n",
    "    max_order=2,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the calculation on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def map_shap_to_features_fixed(shap_values, adatas, sample_idx=0, class_idx=None):\n",
    "    \"\"\"\n",
    "    Map SHAP values back to original feature names\n",
    "    \n",
    "    Args:\n",
    "        shap_values: SHAP values - list of arrays [modality] with shape (samples, features, classes)\n",
    "        adatas: List of AnnData objects for each modality\n",
    "        sample_idx: Which sample to analyze\n",
    "        class_idx: Which class to analyze (if None, uses mean across all classes)\n",
    "    \"\"\"\n",
    "    modality_names = ['RNA', 'Ephys', 'Morph']\n",
    "    results = {}\n",
    "\n",
    "    for mod_idx, (shap_mod, adata, mod_name) in enumerate(zip(shap_values, adatas, modality_names)):\n",
    "        \n",
    "        # shap_mod has shape (n_samples, n_features, n_classes)\n",
    "        if class_idx is not None:\n",
    "            # Use specific class\n",
    "            sample_shap = shap_mod[sample_idx, :, class_idx]  # Shape: (n_features,)\n",
    "        else:\n",
    "            # Average across all classes (mean absolute SHAP values)\n",
    "            sample_shap = np.mean(np.abs(shap_mod[sample_idx, :, :]), axis=1)  # Shape: (n_features,)\n",
    "\n",
    "        # Get feature names\n",
    "        if hasattr(adata, 'var_names'):\n",
    "            feature_names = list(adata.var_names)\n",
    "        elif hasattr(adata, 'var') and 'gene_symbols' in adata.var.columns:\n",
    "            feature_names = list(adata.var['gene_symbols'])\n",
    "        else:\n",
    "            feature_names = [f\"{mod_name}_Feature_{i}\" for i in range(len(sample_shap))]\n",
    "\n",
    "        # Create feature importance list\n",
    "        feature_importance = []\n",
    "        for i, (shap_val, feature_name) in enumerate(zip(sample_shap, feature_names)):\n",
    "            if hasattr(shap_val, 'item'):\n",
    "                shap_scalar = shap_val.item()\n",
    "            else:\n",
    "                shap_scalar = float(shap_val)\n",
    "\n",
    "            feature_importance.append({\n",
    "                'feature_name': feature_name,\n",
    "                'shap_value': shap_scalar,\n",
    "                'abs_shap_value': abs(shap_scalar),\n",
    "                'feature_index': i\n",
    "            })\n",
    "\n",
    "        # Sort by absolute SHAP value\n",
    "        feature_importance.sort(key=lambda x: x['abs_shap_value'], reverse=True)\n",
    "        results[mod_name] = feature_importance\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_top_features_and_data(shap_values, adatas_all, pseudo_label, \n",
    "                              top_rna=100, top_ephys=68, top_morph=100, \n",
    "                              cell_type_prefix='Pvalb'):\n",
    "    \"\"\"\n",
    "    Get top features for each modality and return feature matrix + feature names\n",
    "    \n",
    "    Args:\n",
    "        shap_values: SHAP values - list of arrays [modality] with shape (samples, features, classes)\n",
    "        adatas_all: List of AnnData objects for each modality\n",
    "        pseudo_label: Cell type labels\n",
    "        top_rna: Number of top RNA features to select\n",
    "        top_ephys: Number of top Ephys features to select\n",
    "        top_morph: Number of top Morph features to select\n",
    "        cell_type_prefix: Cell type prefix to analyze\n",
    "    \n",
    "    Returns:\n",
    "        feature_matrix: numpy array of shape (n_cells, total_features)\n",
    "        feature_names: list of feature names matching the column order\n",
    "        top_features_info: dictionary with detailed feature information\n",
    "        feature_indices: dictionary mapping modality to list of original indices\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get indices of neurons with the specified prefix\n",
    "    cell_type_mask = np.array([label.startswith(cell_type_prefix) for label in pseudo_label])\n",
    "    cell_type_indices = np.where(cell_type_mask)[0]\n",
    "    \n",
    "    if len(cell_type_indices) == 0:\n",
    "        print(f\"No {cell_type_prefix} neurons found!\")\n",
    "        return None, None, None\n",
    "    \n",
    "    print(f\"Found {len(cell_type_indices)} {cell_type_prefix} neurons\")\n",
    "    \n",
    "    # Show the subtypes found\n",
    "    subtypes = set([label for label in pseudo_label if label.startswith(cell_type_prefix)])\n",
    "    print(f\"Subtypes: {subtypes}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Analyze each neuron and aggregate results\n",
    "    all_features = {mod: [] for mod in ['RNA', 'Ephys', 'Morph']}\n",
    "    \n",
    "    for neuron_idx in cell_type_indices:\n",
    "        # Get SHAP mapping for this neuron using the fixed function\n",
    "        mapped_shap = map_shap_to_features_fixed(shap_values, adatas_all, \n",
    "                                               sample_idx=neuron_idx, class_idx=None)\n",
    "        \n",
    "        # Collect features from each modality\n",
    "        for modality in mapped_shap:\n",
    "            all_features[modality].extend(mapped_shap[modality])\n",
    "    \n",
    "    # Aggregate features by taking mean across neurons\n",
    "    aggregated_features = {}\n",
    "    top_features_per_modality = {}\n",
    "    \n",
    "    for modality in all_features:\n",
    "        if not all_features[modality]:\n",
    "            continue\n",
    "            \n",
    "        feature_groups = {}\n",
    "        for feature_data in all_features[modality]:\n",
    "            feature_name = feature_data['feature_name']\n",
    "            if feature_name not in feature_groups:\n",
    "                feature_groups[feature_name] = []\n",
    "            feature_groups[feature_name].append(feature_data['abs_shap_value'])\n",
    "        \n",
    "        aggregated = []\n",
    "        for feature_name, shap_values_list in feature_groups.items():\n",
    "            mean_shap = np.mean(shap_values_list)\n",
    "            aggregated.append({\n",
    "                'feature_name': feature_name,\n",
    "                'mean_abs_shap': mean_shap,\n",
    "                'n_neurons': len(shap_values_list)\n",
    "            })\n",
    "        \n",
    "        aggregated.sort(key=lambda x: x['mean_abs_shap'], reverse=True)\n",
    "        aggregated_features[modality] = aggregated\n",
    "    \n",
    "    # Select top features for each modality\n",
    "    top_counts = {'RNA': top_rna, 'Ephys': top_ephys, 'Morph': top_morph}\n",
    "    \n",
    "    for modality in ['RNA', 'Ephys', 'Morph']:\n",
    "        if modality in aggregated_features:\n",
    "            top_n = top_counts[modality]\n",
    "            top_features_per_modality[modality] = aggregated_features[modality][:top_n]\n",
    "            \n",
    "            print(f\"\\nTop {top_n} features for {modality} ({cell_type_prefix}-specific):\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            for i, feature in enumerate(top_features_per_modality[modality]):\n",
    "                print(f\"{i+1:2d}. {feature['feature_name']:25s} | \"\n",
    "                      f\"Mean |SHAP|: {feature['mean_abs_shap']:8.4f}\")\n",
    "    \n",
    "    # Now extract the actual feature data for all cells\n",
    "    total_cells = len(pseudo_label)\n",
    "    total_features = sum(top_counts.values())\n",
    "    \n",
    "    # Initialize feature matrix\n",
    "    feature_matrix = np.zeros((total_cells, total_features))\n",
    "    feature_names = []\n",
    "    feature_indices = {}  # Store original indices for each modality\n",
    "    \n",
    "    # Get the original feature indices for selected top features\n",
    "    feature_col_idx = 0\n",
    "    \n",
    "    for mod_idx, modality in enumerate(['RNA', 'Ephys', 'Morph']):\n",
    "        if modality not in top_features_per_modality:\n",
    "            continue\n",
    "            \n",
    "        adata = adatas_all[mod_idx]\n",
    "        feature_indices[modality] = []\n",
    "        \n",
    "        # Get feature names from adata\n",
    "        if hasattr(adata, 'var_names'):\n",
    "            all_feature_names = list(adata.var_names)\n",
    "        elif hasattr(adata, 'var') and 'gene_symbols' in adata.var.columns:\n",
    "            all_feature_names = list(adata.var['gene_symbols'])\n",
    "        else:\n",
    "            all_feature_names = [f\"{modality}_Feature_{i}\" for i in range(adata.shape[1])]\n",
    "        \n",
    "        # For each top feature, find its index and extract data\n",
    "        for feature_info in top_features_per_modality[modality]:\n",
    "            feature_name = feature_info['feature_name']\n",
    "            \n",
    "            # Find the original feature index\n",
    "            try:\n",
    "                original_idx = all_feature_names.index(feature_name)\n",
    "                feature_indices[modality].append(original_idx)\n",
    "            except ValueError:\n",
    "                # If feature name not found, skip\n",
    "                print(f\"Warning: Feature {feature_name} not found in {modality} data\")\n",
    "                continue\n",
    "            \n",
    "            # Extract feature values for all cells\n",
    "            if hasattr(adata, 'X'):\n",
    "                if hasattr(adata.X, 'toarray'):\n",
    "                    feature_values = adata.X.toarray()[:, original_idx]\n",
    "                else:\n",
    "                    feature_values = adata.X[:, original_idx]\n",
    "            else:\n",
    "                print(f\"Warning: No data matrix found for {modality}\")\n",
    "                feature_values = np.zeros(total_cells)\n",
    "            \n",
    "            # Add to feature matrix\n",
    "            feature_matrix[:, feature_col_idx] = feature_values\n",
    "            feature_names.append(f\"{modality}_{feature_name}\")\n",
    "            feature_col_idx += 1\n",
    "    \n",
    "    print(f\"\\nCreated feature matrix: {feature_matrix.shape}\")\n",
    "    print(f\"Total features: {len(feature_names)}\")\n",
    "    print(f\"Expected features: {total_features}\")\n",
    "    \n",
    "    return feature_matrix, feature_names, top_features_per_modality, feature_indices\n",
    "\n",
    "\n",
    "def analyze_pvalb_features_matrix(shap_values, adatas_all, pseudo_label, \n",
    "                                 cell_type_prefix='Pvalb', \n",
    "                                 top_rna=100, top_ephys=68, top_morph=100):\n",
    "    \"\"\"\n",
    "    Wrapper function that returns the feature matrix and names for the specified cell type\n",
    "    \n",
    "    Returns:\n",
    "        feature_matrix: numpy array of shape (n_cells, 268)\n",
    "        feature_names: list of 268 feature names\n",
    "        feature_indices: dictionary mapping modality to original indices\n",
    "    \"\"\"\n",
    "    \n",
    "    feature_matrix, feature_names, top_features_info, feature_indices = get_top_features_and_data(\n",
    "        shap_values, adatas_all, pseudo_label,\n",
    "        top_rna=top_rna, top_ephys=top_ephys, top_morph=top_morph,\n",
    "        cell_type_prefix=cell_type_prefix\n",
    "    )\n",
    "    \n",
    "    return feature_matrix, feature_names, feature_indices\n",
    "\n",
    "\n",
    "def create_submodel_wrapper_with_feature_mapping(clustering_submodel, feature_indices, adatas_all,\n",
    "                                                original_modality_dims=[1252, 68, 514],\n",
    "                                                selected_modality_dims=[100, 68, 100]):\n",
    "    \"\"\"\n",
    "    Create a wrapper that maps selected features back to original feature space using feature means as baseline\n",
    "    \n",
    "    Args:\n",
    "        clustering_submodel: Your original clustering model\n",
    "        feature_indices: Dictionary with original feature indices for each modality\n",
    "        adatas_all: List of AnnData objects to compute feature means from\n",
    "        original_modality_dims: Original dimensions [RNA, Ephys, Morph]\n",
    "        selected_modality_dims: Selected dimensions [RNA, Ephys, Morph]\n",
    "    \n",
    "    Returns:\n",
    "        Wrapper function that takes selected features and returns predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pre-compute feature means for each modality\n",
    "    modality_means = {}\n",
    "    modality_names = ['RNA', 'Ephys', 'Morph']\n",
    "    \n",
    "    print(\"Computing feature means for baseline...\")\n",
    "    for mod_idx, (modality, adata) in enumerate(zip(modality_names, adatas_all)):\n",
    "        if hasattr(adata, 'X'):\n",
    "            if hasattr(adata.X, 'toarray'):\n",
    "                data_matrix = adata.X.toarray()\n",
    "            else:\n",
    "                data_matrix = adata.X\n",
    "            modality_means[modality] = np.mean(data_matrix, axis=0)\n",
    "        else:\n",
    "            modality_means[modality] = np.zeros(original_modality_dims[mod_idx])\n",
    "        print(f\"{modality} means computed: shape {modality_means[modality].shape}\")\n",
    "    \n",
    "    def submodel_wrapper(X):\n",
    "        \"\"\"\n",
    "        Wrapper to make submodel_clus compatible with SHAPIQ\n",
    "        X: numpy array of shape (n_samples, total_selected_features) - only top features\n",
    "        Returns: numpy array of predictions\n",
    "        \"\"\"\n",
    "        import torch\n",
    "        \n",
    "        batch_size = X.shape[0]\n",
    "        full_modalities = []\n",
    "        \n",
    "        # Split the selected features back into modalities\n",
    "        selected_start_idx = 0\n",
    "        for mod_idx, (modality, orig_dim, sel_dim) in enumerate(zip(modality_names, \n",
    "                                                                    original_modality_dims, \n",
    "                                                                    selected_modality_dims)):\n",
    "            \n",
    "            # Start with feature means as baseline (broadcast to batch size)\n",
    "            full_modality_data = np.tile(modality_means[modality], (batch_size, 1))\n",
    "            \n",
    "            # Extract selected features for this modality\n",
    "            selected_end_idx = selected_start_idx + sel_dim\n",
    "            selected_features = X[:, selected_start_idx:selected_end_idx]\n",
    "            \n",
    "            # Map selected features back to original positions\n",
    "            if modality in feature_indices:\n",
    "                original_indices = feature_indices[modality]\n",
    "                for i, orig_idx in enumerate(original_indices):\n",
    "                    if i < selected_features.shape[1]:  # Safety check\n",
    "                        full_modality_data[:, orig_idx] = selected_features[:, i]\n",
    "            \n",
    "            # Convert to torch tensor\n",
    "            full_modalities.append(torch.tensor(full_modality_data, dtype=torch.float32, \n",
    "                                              device=torch.device('mps')))\n",
    "            \n",
    "            selected_start_idx = selected_end_idx\n",
    "        \n",
    "        # Run through the clustering submodel\n",
    "        clustering_submodel.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = clustering_submodel(*full_modalities)\n",
    "            \n",
    "            # Convert to numpy\n",
    "            if hasattr(predictions, 'cpu'):\n",
    "                predictions = predictions.cpu().numpy()\n",
    "            \n",
    "            return predictions\n",
    "    \n",
    "    return submodel_wrapper\n",
    "\n",
    "\n",
    "def get_computationally_feasible_features(shap_values, adatas_all, pseudo_label,\n",
    "                                        total_budget_features=20,\n",
    "                                        cell_type_prefix='Pvalb'):\n",
    "    \"\"\"\n",
    "    Get a computationally feasible subset of top features while maintaining model performance\n",
    "    \n",
    "    Args:\n",
    "        total_budget_features: Total number of features to select (should be  25 for 2^n feasibility)\n",
    "        \n",
    "    Returns:\n",
    "        feature_matrix: numpy array of shape (n_cells, total_budget_features)\n",
    "        feature_names: list of feature names\n",
    "        feature_indices: mapping back to original indices\n",
    "        modality_allocation: how features were distributed across modalities\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the full top features analysis first\n",
    "    full_feature_matrix, full_feature_names, top_features_info, full_feature_indices = get_top_features_and_data(\n",
    "        shap_values, adatas_all, pseudo_label,\n",
    "        top_rna=100, top_ephys=68, top_morph=100,  # Get the full analysis\n",
    "        cell_type_prefix=cell_type_prefix\n",
    "    )\n",
    "    \n",
    "    # Now intelligently allocate the budget across modalities\n",
    "    # Proportional to original importance, but ensure each modality gets at least some features\n",
    "    \n",
    "    # Count how many features we actually have per modality\n",
    "    modality_counts = {}\n",
    "    for modality in ['RNA', 'Ephys', 'Morph']:\n",
    "        if modality in top_features_info:\n",
    "            modality_counts[modality] = len(top_features_info[modality])\n",
    "        else:\n",
    "            modality_counts[modality] = 0\n",
    "    \n",
    "    print(f\"Available features per modality: {modality_counts}\")\n",
    "    \n",
    "    # Allocate budget proportionally, but ensure minimum representation\n",
    "    min_per_modality = max(1, total_budget_features // 10)  # At least 10% each, minimum 1\n",
    "    remaining_budget = total_budget_features - (3 * min_per_modality)\n",
    "    \n",
    "    # Distribute remaining budget proportionally\n",
    "    total_available = sum(modality_counts.values())\n",
    "    allocation = {}\n",
    "    \n",
    "    for modality in ['RNA', 'Ephys', 'Morph']:\n",
    "        if total_available > 0:\n",
    "            proportion = modality_counts[modality] / total_available\n",
    "            extra = int(remaining_budget * proportion)\n",
    "            allocation[modality] = min_per_modality + extra\n",
    "        else:\n",
    "            allocation[modality] = min_per_modality\n",
    "    \n",
    "    # Adjust if we went over budget\n",
    "    total_allocated = sum(allocation.values())\n",
    "    if total_allocated > total_budget_features:\n",
    "        # Reduce proportionally\n",
    "        scale_factor = total_budget_features / total_allocated\n",
    "        for modality in allocation:\n",
    "            allocation[modality] = max(1, int(allocation[modality] * scale_factor))\n",
    "    \n",
    "    print(f\"Feature allocation: {allocation} (total: {sum(allocation.values())})\")\n",
    "    \n",
    "    # Select the top N features from each modality\n",
    "    selected_features = {}\n",
    "    selected_feature_indices = {}\n",
    "    \n",
    "    for modality in ['RNA', 'Ephys', 'Morph']:\n",
    "        if modality in top_features_info and allocation[modality] > 0:\n",
    "            n_select = min(allocation[modality], len(top_features_info[modality]))\n",
    "            selected_features[modality] = top_features_info[modality][:n_select]\n",
    "            selected_feature_indices[modality] = full_feature_indices[modality][:n_select]\n",
    "    \n",
    "    # Build the reduced feature matrix\n",
    "    total_cells = len(pseudo_label)\n",
    "    actual_total_features = sum(len(selected_features.get(mod, [])) for mod in ['RNA', 'Ephys', 'Morph'])\n",
    "    \n",
    "    reduced_feature_matrix = np.zeros((total_cells, actual_total_features))\n",
    "    reduced_feature_names = []\n",
    "    reduced_feature_indices = {}\n",
    "    \n",
    "    col_idx = 0\n",
    "    for mod_idx, modality in enumerate(['RNA', 'Ephys', 'Morph']):\n",
    "        if modality not in selected_features:\n",
    "            reduced_feature_indices[modality] = []\n",
    "            continue\n",
    "            \n",
    "        adata = adatas_all[mod_idx]\n",
    "        reduced_feature_indices[modality] = []\n",
    "        \n",
    "        for feature_info in selected_features[modality]:\n",
    "            feature_name = feature_info['feature_name']\n",
    "            \n",
    "            # Find the original feature index\n",
    "            if hasattr(adata, 'var_names'):\n",
    "                all_feature_names = list(adata.var_names)\n",
    "            elif hasattr(adata, 'var') and 'gene_symbols' in adata.var.columns:\n",
    "                all_feature_names = list(adata.var['gene_symbols'])\n",
    "            else:\n",
    "                all_feature_names = [f\"{modality}_Feature_{i}\" for i in range(adata.shape[1])]\n",
    "            \n",
    "            try:\n",
    "                original_idx = all_feature_names.index(feature_name)\n",
    "                reduced_feature_indices[modality].append(original_idx)\n",
    "                \n",
    "                # Extract feature values\n",
    "                if hasattr(adata.X, 'toarray'):\n",
    "                    feature_values = adata.X.toarray()[:, original_idx]\n",
    "                else:\n",
    "                    feature_values = adata.X[:, original_idx]\n",
    "                \n",
    "                reduced_feature_matrix[:, col_idx] = feature_values\n",
    "                reduced_feature_names.append(f\"{modality}_{feature_name}\")\n",
    "                col_idx += 1\n",
    "                \n",
    "            except ValueError:\n",
    "                print(f\"Warning: Feature {feature_name} not found in {modality}\")\n",
    "    \n",
    "    # Update the modality dimensions for the reduced set\n",
    "    reduced_modality_dims = [len(reduced_feature_indices.get(mod, [])) for mod in ['RNA', 'Ephys', 'Morph']]\n",
    "    \n",
    "    print(f\"\\nReduced feature matrix: {reduced_feature_matrix.shape}\")\n",
    "    print(f\"Modality dimensions: {reduced_modality_dims}\")\n",
    "    print(f\"2^{actual_total_features} = {2**actual_total_features:,} combinations (feasible: {2**actual_total_features < 1e8})\")\n",
    "    \n",
    "    return reduced_feature_matrix, reduced_feature_names, reduced_feature_indices, reduced_modality_dims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get computationally feasible feature subset\n",
    "reduced_feature_matrix, reduced_feature_names, reduced_feature_indices, reduced_modality_dims = get_computationally_feasible_features(\n",
    "    shap_values, _, all_pseudo_label,\n",
    "    total_budget_features=20,  # Adjust based on your computational budget\n",
    "    cell_type_prefix='Pvalb'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create wrapper that maintains full model performance\n",
    "submodel_wrapper = create_submodel_wrapper_with_feature_mapping(\n",
    "    clustering_submodel=clustering_submodel,\n",
    "    feature_indices=reduced_feature_indices,\n",
    "    adatas_all=_,\n",
    "    original_modality_dims=[1252, 68, 514],     # Your original dimensions\n",
    "    selected_modality_dims=reduced_modality_dims  # The reduced dimensions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = reduced_feature_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiq_approximator_2_order = KernelSHAPIQ(n=n, max_order=2, index=\"k-SII\")\n",
    "# Step 3: Run SHAPIQ (now computationally feasible!)\n",
    "explainer = shapiq.TabularExplainer(\n",
    "    model=submodel_wrapper,\n",
    "    data=reduced_feature_matrix,  # Shape: (n_cells, ~20)\n",
    "    approximator=shapiq_approximator_2_order,\n",
    "    index=\"k-SII\",\n",
    "    max_order=2,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prediction variance\n",
    "preds = submodel_wrapper(reduced_feature_matrix[:100])\n",
    "print(f\"Prediction std: {np.std(preds)}\")\n",
    "print(f\"Prediction range: [{np.min(preds):.6f}, {np.max(preds):.6f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = explainer.explain(x=reduced_feature_matrix[5], budget = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations = explainer.explain_X(reduced_feature_matrix, budget=10000, n_jobs=8, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.get_top_k_interactions(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiq.plot.bar_plot([a], feature_names=reduced_feature_names, show=True, abbreviate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.plot_si_graph(feature_names=reduced_feature_names, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = shapiq.plot.stacked_bar_plot(\n",
    "    interaction_values=a,\n",
    "    feature_names=reduced_feature_names,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiq.plot.si_graph_plot(interaction_values=a, feature_names=reduced_feature_names, show=True, compactness=10, n_interactions=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.plot_network(feature_names=reduced_feature_names, show=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
